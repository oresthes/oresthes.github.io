---
layout: post
title:  "Generalized Linear Models"
date:   2017-12-25 13:07:53 -0600
categories: jekyll update
---


This notebook discusses the theory behind the Generalized Linear Model.  
This is an an extraction of material and examples from various sources I have come across and found useful over time.

Sources:  

1. *Faraway* - Extending the Linear Model with R
2. *Nelder, McCullagh* - Generalized Linear Models 



# Preamble 

Before discussing GLM, a brief overview of the processes associated with model fitting.  

## Model Selection 

Models must be selected from a particular class, which has to be relevant to the kind of data under study. Assumptions involved regarding the data that is used with GLM are listed below:  

- Independent or at least uncorrelated observations. This excludes time series and spatial processes data, which exhibit autocorrelation.  
- A single error term is allowed in the error structure.  

Regarding the choice of scale for analysis, we might need to keep in mind certain characteristics (by scale, we can consider using original scale (Y) or modified, (log(Y) or sqrt(Y) etc.). For **classical linear regression**, a good scale should contain:  

- Constancy of Variance  
- Approximate Normality of Errors
- Additivity of systematic effects  

This scale need not exist, also different scales might satisfy the different conditions above.  

GLMs reduces greatly scaling problems.  
- Constancy of Variance and Normality are **not required**  
- Dependence of Variance on the mean must be known 
- Additivity of effects is required. It can be specified to hold on a transformed scale.  

On the other hand, we have to deal with the choice of covariates, x-variables, to be included in the systematic part of the model.  In general out of p covariates, we are required to find a subset of these that is in some sense best for constructing the fitted values **$\hat\mu$**:  

$$
\hat\mu_i = \sum_j x_{ij}\hat\beta_{j}
$$

## Estimation  

After a particular model has been selected, it is required to estimate parameters and assess the precision of the estimates.  
- For GLM, estimation proceeds by defining a measure of goodness of fit between the observed data and the fitted values generated by the model.   
- The parameter estimates are the values that minimize this goodness of fit criterion.  

If $f(y;\theta)$ is the pdf for observation $y$ given parameter $\theta$, then the log likelihood, expressed as a function of the mean value parameter, $\mu = E(Y)$, is simply:  

$$
l(\mu;y) = log \, f(y;\theta)
$$
The log likelihood based on a set of independent observations $y_1...y_n$, is simply the sum of individual contributions:

$$
l(\boldsymbol{\mu;y}) = \sum_ilog \,f_i(y_i;\theta_i)
$$
Notice the subtle difference between the definitions of the density function and the log likelihood function:  
- The density function $f(y;\theta)$ is considered as a function of $y$ for a fixed $\theta$.  
- The log-likelihood function $l(\mu;y)$ is considered primarily as a function of $\theta$ (i.e. $\mu$) for the particular data $y$ being observed. 

Using the above defined log-likelihood function, we can define as a goodness-of-fit function:  

$$
D^*(\boldsymbol{y;\mu}) = 2l(\boldsymbol{y;y}) - 2l(\boldsymbol{\mu;y})
$$
This function is called **scaled deviance**. Note that $l(\boldsymbol{y;y})$ is the maximum likelihood achievable since it is an exact fit (fitted values = observed values) for the particular class of exponential-family models considered.  

The objective therefore is to minimize $D^*(\boldsymbol{y;\mu})$ with respect to $\boldsymbol{\mu}$. 

It can be shown that for normal-theory linear regression models with known variance $\sigma^2$, the scaled deviance function is identical to the residual sum of squares up to a constant factor $\sigma^2$. Therefore the problem of minimizing the scaled deviance is equaivalent to the least squares problem.  

# Generalized Linear Models  

GLMs are an extension of classical linear models, which makes the latter an excellent starting point. 

The classical linear model can be summarized in the form:  

The components of **Y** are *independent*, *Normal* variables with *constant variance* $\sigma^2$ and:  

$$
E(\boldsymbol{Y}) = \boldsymbol{\mu} \qquad where \qquad \boldsymbol{\mu = X\beta}
$$

## Generalization 

We can devise a three-part specification as follows: 

1. The *random component*: components of **Y** have *independent, normal, distributions* with E[**Y**] = $\boldsymbol{\mu}$ and *constant variance* $\sigma^2$.  
2. The *systematic component:* covariates $\boldsymbol{x_1,x_2...,x_p}$ produce a linear predictor given by:  

$$
\boldsymbol{\eta} = \sum_{j=1}^{p}\boldsymbol{x}_j\beta_j
$$
3. The *link* between the random and systematic components: 

$$
\boldsymbol{\mu}= \boldsymbol{\eta} \equiv g(\boldsymbol{\mu})
$$
where g is called the *link function*. 

In the classic linear model, we have a Gaussian distribution for component 1, and the identity function as a link for component 3.  

Generalized Linear Models allow two extensions at this point:  

1. The distribution in component 1 may come from an *exponential family* other than normal.  
2. The link function in component 3 may become *any monotonic differentiable function*.  

### Exponential Family of Distributions:  

Each component of $Y$ has a distribution in the exponential family, which takes the general form:  

$$
f(y|\theta,\phi) = exp\left[ \frac{y\theta-b(\theta)}{a(\phi)} + c(y,\phi)\right]
$$

where  

- $\theta$ is called the canonical parameter and represents location.  
- $\phi$ is called the dispersion parameter and represents scale.  
- a, b, c are also functions defining the distribution.   
Few common ones include:  

### 1. Normal(Gaussian)  


$$
f(y|\theta,\phi) = \frac{1}{\sqrt{2\pi\sigma}}exp\left[ -\frac{(y-\mu)^2}{2\sigma^2}\right]
$$
can be rewritten in standard form as:

$$
f(y|\theta,\phi) = exp\left[\frac{y\mu -\frac{\mu^2}{2}}{\sigma^2} - \frac{1}{2} \left(\frac{y^2}{\sigma^2} + log(2\pi\sigma^2) \right) \right]
$$
In this form we can easily identify:

- $\theta = \mu$  
- $\phi = \sigma^2$  
- $a(\phi) = \phi$  
- $b(\theta) = \theta^2/2$
- $c(y,\phi) = -\frac{1}{2} \left( \frac{y^2}{\phi} + log(2\pi\phi) \right)$


###2. Poisson  

$$
f(y|\theta,\phi) = e^{-\mu}\frac{\mu^y}{y!}
$$
can be rewritten in standard form as:  

$$
f(y|\theta,\phi) = exp(y \, log(\mu) - \mu - log(y!))
$$

In this form we can easily identify  

- $\theta = log(\mu)$  
- $\phi = 1$  
- $a(\phi) = 1$  
- $b(\theta) = exp(\theta)$   
- $c(y,\phi) = -log(y!)$


And we can similarly define other exponential distributions. Notice a difference between the two distributions shown here:  

- The Normal Density has a free $\phi$ parameter. The *Gamma* distribution has a similar composition. The family for these distributions is sometimes called the *exponential dispersion family*.
- *Poisson* has a fixed $\phi$ parameter. Poisson is part of the one parameter families which also includes the *binomial* distribution. The family is sometimes exclusively called the *exponential family* distribution to contrast it with the above category.    

### Derivation of the mean and variance of the exponential family distributions.  

The first and second derivatives with respect to $\theta$ are:  

$$
\frac{\partial l}{\partial \theta} = \frac{y-b'(\theta)}{a(\phi)}\\
\frac{\partial^2l}{\partial\theta^2} = \frac{-b''(\theta)}{a(\phi)}
$$
From general likelihood theory we have the following two relations (cit?):  

1. Expectation of first derivative

$$
E\left(\frac{\partial l}{\partial \theta}\right) = 0\\
\frac{E(Y) - b'(\theta)}{a(\phi)} = 0\\
$$
Hence, we can derive the mean as:

$$
E(Y) = \mu = b'(\theta)
$$

2. Expectation of the second derivative  

$$
E\left(\frac{\partial^2l}{\partial\theta^2}\right) + E\left(\frac{\partial l}{\partial \theta}\right)^2 = 0 \\
-\frac{b''(\theta)}{a(\phi)} + \frac{E[(Y - E(Y))^2]}{a^2(\phi)} = 0
$$
Hence, we can derive the variance as:  

$$
var(Y) = b''(\theta)\cdot a(\phi)
$$
Variance of Y is a product of two functions:   
- $b''(\theta)$ which depends on the canonical parameter $\theta$ (hence, the mean or the location). This will be called the *variance function*. The variance function is typically written as a function of $\mu$ using the known relationship between $\theta$ and $\mu$.  
Note that for Gaussian distribution $b''(\theta) = 1$, hence variance does not depend on the mean. This makes the Gaussian distribution exceptional.  
- $a(\phi)$ which is indpendent of $\theta$ and depends only on $\phi$.   

## Link Functions  

Link functions relate the linear predictor $\eta$ to the expected value $\mu$ of a random variable y. For classical linear models, the mean and predictor are identical, and the identity link is plausible in that both $\eta$ and $\mu$ can take any value on the real line.  
However, when we are dealing with counts with a Poisson distribution, we need to have $\mu >0$. In this case the identity link is less desirable since $\eta$ can take on negative values, while $\mu$ cannot. Additionally, models for counts based on independence in cross-classified data lead naturally to multiplicative effects, which is expressed by the log link, $\eta = log \mu$, with its inverse, $\mu = e^\eta$. No additive effects contributing to $\eta$ become multiplicative effects contributing to $\mu$.  
Similar arguments can be put for other distributions. 

###Canonical link  

The *canonical link* has a g such that:  
$$\eta = g(\mu) = \theta$$
This means that:

$$ g(b'(\theta))= \theta$$


## Fitting the GLM Model  

We can estimate the parameters $\Beta$ using the maximum likelihood method. The log-likelihood for a single observations, where $a_i(\phi)$ can be represented as $\phi/w_i$.  

$$
l(\beta;y_i) = w_i \left[\frac{y_i\theta_i - b(\theta_i)}{\phi} \right] + c(y_i,\phi)
$$
For **independent observations**, the log-likelihood will be $\sum_i l(\beta;y_i)$
